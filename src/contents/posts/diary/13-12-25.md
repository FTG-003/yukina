---

title: The Illusion of Knowing in the Age of Artificial Intelligence
published: 2025-12-13
description: How AI-generated answers can simulate knowledge — and how Pyragogy responds to the illusion of understanding.
author: Fabrizio Terzi
bannerImage: "/banner/forum-square.webp"
category: Research
tags: [AI, Learning, Cognition, Pyragogy, Diary]
draft: false
------------

## The Illusion of Knowing in the Age of Artificial Intelligence

There is a word that captures our current relationship with artificial intelligence better than many technical analyses: **epistemia**.
It is not ignorance.
It is not misinformation.

It is something more subtle and more dangerous: **the illusion of knowing**, produced by answers that *sound* correct, coherent, and authoritative.

In ancient Greek philosophy, *episteme* referred to grounded, justified knowledge.
*Epistemia* is its simulacrum: knowledge that appears solid because it is well articulated, but that lacks a genuine process of understanding.

Contemporary artificial intelligence excels precisely at this: **creating the impression of knowledge**.

<p align="center">
  <img src="/images/greca.png" alt="Greca Divider" />
</p>

---

## A small, everyday example

Imagine a very common situation.
You ask a chatbot to explain a concept you feel you “should” already know — a regulation, a scientific idea, a historical event. The response arrives quickly: fluent, structured, reassuring. You read it and nod. It seems clear.

Now imagine someone immediately asks you:
*“Can you explain it in your own words?”*
or
*“Where does this information come from?”*

You hesitate. You glance back at the AI’s response. You realize that **you haven’t really made it your own**.
You didn’t misunderstand it.
You simply **mistook clarity of language for understanding**.

This is epistemia at work.

---

## AI does not know. It generates plausibility.

Large language models are not designed to know the world, verify facts, or understand meaning.
They are designed to **generate linguistically plausible outputs**.

This is not a flaw. It is their nature.
The problem arises when **plausibility is mistaken for truth**.

We ask AI systems for:

* judgments
* evaluations
* classifications
* summaries
* decisions

And there is nothing inherently wrong with that.
The critical moment comes **after** the response, when the output is accepted without friction, without verification, without reflection.

That is where delegation turns into abdication.

---

## From Socrates to LLMs: persuasion without knowledge

In fifth‑century Athens, Socrates accused the sophists of crafting persuasive discourse without truth. Gorgias famously argued that nothing exists, that if it existed it could not be known, and that if it could be known it could not be communicated.

Artificial intelligence does almost the opposite:
**it can communicate everything, without knowing anything**.

Yet the outcome is similar:
a discourse that convinces not because it is true, but because it is well constructed.

The crucial difference today is that **persuasion no longer has intention**.
AI does not aim to deceive.
But the cognitive effects on humans are real.

---

## Epistemia as a structural phenomenon (what research shows)

A recent study published in *Proceedings of the National Academy of Sciences* (PNAS), led by Walter Quattrociocchi and colleagues at Sapienza University of Rome, examined how state‑of‑the‑art language models evaluate reliability and credibility.

Crucially, the authors show that large language models do not reproduce human judgment — they **simulate its form**.

As the paper states, language models tend to generate *“the simulation of judgment, rather than judgment itself”*, producing evaluations that are linguistically convincing while being grounded in statistical regularities rather than epistemic validation.

The focus of the study is not whether answers are right or wrong, but **how judgments are constructed**.

This is where epistemia emerges:

* coherent evaluations
* confident language
* convincing argumentative structures

built on **epistemically fragile foundations**.

The danger is not occasional error.
The danger is **becoming accustomed to the surface**.

---

## Pyragogy: a design response, not a moral lecture

Saying “we need more critical thinking” is correct — but insufficient.
Critical thinking does not arise spontaneously, nor by good intentions alone.

**Pyragogy** starts from a different premise:

> if learning environments are poorly designed, the illusion of knowing becomes inevitable.

For this reason, Pyragogy does not ask humans to “trust AI less.”
It asks us to **redesign the role of AI within the cognitive process**.

In a pyragogical context:

* AI does not merely provide answers
* it makes reasoning steps visible
* it exposes uncertainty
* it invites comparison
* it introduces cognitive friction

It does not replace thinking.
It **activates** it.

---

## A peeragogical add‑on: thinking is not a solo act

There is one more layer.
Epistemia does not grow only in individuals — it grows in isolation.

When AI replaces dialogue, when answers replace discussion, when fluency replaces disagreement, **thinking collapses into consumption**.

Peeragogy — and Pyragogy with it — reminds us that knowledge is not produced alone.
It emerges through **shared inquiry, mutual correction, and distributed responsibility**.

AI can participate in this ecology.
But it cannot replace it.

---

## Responsibility remains human (and shared)

Fake news, deepfakes, and epistemia are not neutralized by watermarks or warning labels.
They are addressed by designing **learning ecosystems** in which:

* verification is part of the process
* doubt is legitimate
* knowledge is understood as a path, not an output

AI can amplify stupidity, if we ask it to.
But it can also become a cognitive ally — if we accept the higher cost:
**remaining responsible for our own thinking**.

The real question, then, is not whether AI will make us smarter or more ignorant.
The question is whether we are willing to **rethink how we learn — together**.

And today, that choice is still entirely human.

---

## References

Loru, E., Nudo, J., Di Marco, N., Cinelli, M., & Quattrociocchi, W. (2025).
*The simulation of judgment in large language models*.
**Proceedings of the National Academy of Sciences (PNAS)**.
[https://www.pnas.org/doi/epdf/10.1073/pnas.2518443122](https://www.pnas.org/doi/epdf/10.1073/pnas.2518443122)
